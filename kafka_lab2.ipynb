{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiynxczXJ7Q5"
      },
      "source": [
        "# Worked Examples & Exercises - Part 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEzjy63RJ7Q6"
      },
      "source": [
        "## Example 6: Create Another Kafka Topic"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bdXzFc1TKJZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQTJZdQllrdB"
      },
      "outputs": [],
      "source": [
        "#Download Kafka\n",
        "!wget https://downloads.apache.org/kafka/3.9.0/kafka_2.12-3.9.0.tgz\n",
        "!tar -xzf kafka_2.12-3.9.0.tgz\n",
        "!sudo mv kafka_2.12-3.9.0 /usr/local/kafka"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0d3y1nF-lrdC"
      },
      "outputs": [],
      "source": [
        "#Set Environment Variables\n",
        "!echo \"export KAFKA_HOME=/usr/local/kafka\" >> ~/.bashrc\n",
        "!echo \"export PATH=\\$PATH:\\$KAFKA_HOME/bin\" >> ~/.bashrc\n",
        "\n",
        "# Export for current session\n",
        "import os\n",
        "os.environ[\"KAFKA_HOME\"] = \"/usr/local/kafka\"\n",
        "os.environ[\"PATH\"] = os.environ[\"PATH\"] + \":\" + os.environ[\"KAFKA_HOME\"] + \"/bin\"\n",
        "\n",
        "print(\"Environment variables set for this session.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Install Zookeeper\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install -y zookeeper"
      ],
      "metadata": {
        "id": "3CN9Y-AwKpWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Set Zookeeper Environment Variables\n",
        "!echo \"export ZOOKEEPER_HOME=/usr/share/zookeeper\" >> ~/.bashrc\n",
        "!echo \"export PATH=\\$PATH:\\$ZOOKEEPER_HOME/bin\" >> ~/.bashrc\n",
        "\n",
        "# Export for current session\n",
        "import os\n",
        "os.environ[\"ZOOKEEPER_HOME\"] = \"/usr/share/zookeeper\"\n",
        "os.environ[\"PATH\"] = os.environ[\"PATH\"] + \":\" + os.environ[\"ZOOKEEPER_HOME\"] + \"/bin\"\n",
        "\n",
        "print(\"Zookeeper environment variables set for this session.\")"
      ],
      "metadata": {
        "id": "HbPwCbf2KzMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Start Kafka and Zookeeper\n",
        "# Start Zookeeper first\n",
        "!sudo /usr/share/zookeeper/bin/zkServer.sh start\n",
        "\n",
        "# Add a short delay to ensure Zookeeper is fully started\n",
        "!sleep 5\n",
        "\n",
        "# Start Kafka using the full path\n",
        "!/usr/local/kafka/bin/kafka-server-start.sh -daemon /usr/local/kafka/config/server.properties\n",
        "\n",
        "# Add a short delay to ensure Kafka is fully started\n",
        "!sleep 5\n",
        "\n",
        "print(\"Zookeeper and Kafka started.\")"
      ],
      "metadata": {
        "id": "q3X4K17SLO_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzfmHzokJ7Q7"
      },
      "outputs": [],
      "source": [
        "#Create WeatherTopic2\n",
        "!/usr/local/kafka/bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 5 --topic weatherTopic-new"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Verify that WeatherTopic2 was created\n",
        "!/usr/local/kafka/bin/kafka-topics.sh --list --bootstrap-server localhost:9092"
      ],
      "metadata": {
        "id": "9QYa7aqTMrRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Edj-2pEJJ7Q7"
      },
      "source": [
        "**What the code does:**\n",
        "- Demonstrates how to create a second topic with 5 partitions.\n",
        "\n",
        "**Exercise 6**:\n",
        "- Create a topic named labWeather with 3 partitions.\n",
        "- **Question**: Which command do you use to confirm its creation?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VqyLFJpOJ7Q8"
      },
      "outputs": [],
      "source": [
        "# Create your labWeather topic here\n",
        "!/usr/local/kafka/bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 5 --topic labWeather"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jl_NZfDJ7Q8"
      },
      "source": [
        "## Example 7: Check Topic Details"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6wpGdI_J7Q8"
      },
      "outputs": [],
      "source": [
        "!/usr/local/kafka/bin/kafka-topics.sh --describe --bootstrap-server localhost:9092 \\  --topic labWeather"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9aSHL2qJ7Q8"
      },
      "source": [
        "**What the code does:**\n",
        "- Shows replication factor, partition count, and leader info for weatherTopic.\n",
        "\n",
        "**Exercise 7**:\n",
        "- Describe your labWeather topic.\n",
        "- **Question**: How many partitions and which broker is the leader?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FTjU06D_J7Q8"
      },
      "outputs": [],
      "source": [
        "# Describe your labWeather topic here\n",
        "!!/usr/local/kafka/bin/kafka-topics.sh --bootstrap-server localhost:9092 localhost:2181 \\\n",
        "  --topic labWeather"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_Y7bW16J7Q9"
      },
      "source": [
        "## Example 8: Spark Structured Streaming from Kafka"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKeEb_0fJ7Q9"
      },
      "outputs": [],
      "source": [
        "# PySpark code\n",
        "# For Scala, use the commented example below in a Spark shell or notebook with Scala kernel\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"KafkaSparkStreaming\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "kafkaDF = spark.readStream \\\n",
        "    .format(\"kafka\") \\\n",
        "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
        "    .option(\"subscribe\", \"weatherTopic\") \\\n",
        "    .load()\n",
        "\n",
        "query = kafkaDF.selectExpr(\"CAST(value AS STRING)\") \\\n",
        "    .writeStream \\\n",
        "    .format(\"console\") \\\n",
        "    .start()\n",
        "\n",
        "query.awaitTermination()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlwgECL-J7Q9"
      },
      "source": [
        "**What the code does:**\n",
        "- Reads streaming data from Kafka into Spark, printing messages to console in real-time.\n",
        "\n",
        "**Exercise 8**:\n",
        "- Stream data from your labWeather topic instead.\n",
        "- **Question**: Does the console immediately show the messages you produce?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fi8gopzEJ7Q-"
      },
      "outputs": [],
      "source": [
        "# Modify the code to stream from labWeather topic\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"KafkaSparkStreaming\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "kafkaDF = spark.readStream \\\n",
        "    .format(\"kafka\") \\\n",
        "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
        "    .option(\"subscribe\", \"labWeather\") \\\n",
        "    .load()\n",
        "\n",
        "query = kafkaDF.selectExpr(\"CAST(value AS STRING)\") \\\n",
        "    .writeStream \\\n",
        "    .format(\"console\") \\\n",
        "    .start()\n",
        "\n",
        "query.awaitTermination()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p79i7cWrJ7Q-"
      },
      "source": [
        "## Example 9: Checkpointing in Spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFwLp_0tJ7Q-"
      },
      "outputs": [],
      "source": [
        "# PySpark code\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"KafkaSparkStreamingWithCheckpoint\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "kafkaDF = spark.readStream \\\n",
        "    .format(\"kafka\") \\\n",
        "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
        "    .option(\"subscribe\", \"weatherTopic\") \\\n",
        "    .load()\n",
        "\n",
        "query = kafkaDF.selectExpr(\"CAST(value AS STRING)\") \\\n",
        "    .writeStream \\\n",
        "    .option(\"checkpointLocation\", \"hdfs:///checkpoints/kafka-weather\") \\\n",
        "    .format(\"console\") \\\n",
        "    .start()\n",
        "\n",
        "query.awaitTermination()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKOVhc0IJ7Q-"
      },
      "source": [
        "**What the code does:**\n",
        "- Storing offsets in a checkpoint location ensures fault tolerance.\n",
        "\n",
        "**Exercise 9**:\n",
        "- Create a checkpoint location hdfs:///checkpoints/labWeather for your streaming job.\n",
        "- **Question**: In case of failure, can the stream recover from the last known offsets?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlF8Ds_jJ7Q-"
      },
      "outputs": [],
      "source": [
        "# Modify the code to use the labWeather checkpoint\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"KafkaSparkStreamingWithCheckpoint\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "kafkaDF = spark.readStream \\\n",
        "    .format(\"kafka\") \\\n",
        "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
        "    .option(\"subscribe\", \"labWeather\") \\\n",
        "    .load()\n",
        "\n",
        "query = kafkaDF.selectExpr(\"CAST(value AS STRING)\") \\\n",
        "    .writeStream \\\n",
        "    .option(\"checkpointLocation\", \"hdfs:///checkpoints/labWeather\") \\\n",
        "    .format(\"console\") \\\n",
        "    .start()\n",
        "\n",
        "query.awaitTermination()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}